{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbrPXkSGlcmB"
      },
      "source": [
        "#Máster en Big Data y Data Science: Ciencia e Ingeniería de Datos\n",
        "### ASIGNATURA: Indexación, búsqueda y análisis en repositorios multimedia\n",
        "### PARTE: Multimedia (imagen, video)\n",
        "### Práctica 1: Introducción al diseño de redes neuronales convolucionales con Pytorch mediante Google Colaboratory\n",
        "\n",
        "---\n",
        "\n",
        "Autor: Juan C. SanMiguel (juancarlos.sanmiguel@uam.es), Universidad Autónoma de Madrid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7_EXhmb8zwb"
      },
      "source": [
        "# 3. Definición de la red neuronal convolucional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zoOE4V7l5tS"
      },
      "source": [
        "## Preparación del entorno de trabajo\n",
        "\n",
        "A continuación tiene un conjunto de instrucciones que instalan el software necesario para esta parte de la práctica.\n",
        "\n",
        "Recuerde que este código es compatible con Python 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dm_31ZAnl8wl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.0+cu111 (from versions: 0.4.1, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.1.0.post2, 1.2.0, 1.3.0, 1.3.0.post2, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.10.0+cu111\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch==1.10.0+cu111 torchvision==0.11.1+cu111 torchaudio==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vqnHhs1cUOJ"
      },
      "source": [
        "## Red Convolucional Neuronal (CNN)\n",
        "\n",
        "En esta parte vamos a describir los elementos básicos para definir una red neuronal de tipo *feed-forward*. Este tipo de redes toman una serie de datos de entrada (*input*), éstos son procesados por una serie de capas (*layers*) de manera secuencial y finalmente se genera una salida (*output*) relacionada con la tarea a resolver.\n",
        "\n",
        "En general, para definir la mayoría de redes convolucionales necesitamos conocer los siguientes pasos:\n",
        "\n",
        "*   Definir la estructura de la red, que tendrá algunos parámetros entrenables (i.e. *weights*).\n",
        "*   Definir la secuencia de procesado de los datos para obtener una salida (i.e. *forward pass*).\n",
        "*   Calcular la función de pérdidas que indique la precisión de la salida con respecto a nuestras anotaciones (i.e. *loss function*).\n",
        "*   Calcular los gradientes de retropropagación ejecutando en modo inverso la red (i.e. *backward propagation*)\n",
        "*   Actualización de los parámetros de la red acorde a los gradientes anteriores\n",
        "\n",
        "En esta parte, vamos a tomar como ejemplo la red LENET http://yann.lecun.com/exdb/lenet/ cuya estructura se visualiza a continuacion:\n",
        "\n",
        "\n",
        "![alt text](http://pytorch.org/tutorials/_images/mnist.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K05iyB_ykEjj"
      },
      "source": [
        "### Capas de una red (layers)\n",
        "Para definir las capas una red, Pytorch hace uso del paquete ``torch.nn``.\n",
        "\n",
        "Primeramente podemos definir *capas convolucionales 2D* mediante la función ``nn.Conv2d`` que tiene los principales argumentos:\n",
        "*   **in_channels**: valor para la tercera dimension del tensor de entrada (e.g. 3 para imágenes RGB, 1 para imágenes en Gris). \n",
        "*   **out_channels**: número de mapas de salida (i.e. número de convoluciones o *kernels* que aplicamos sobre los datos de entrada).\n",
        "*   **kernel_size**: tamaño del *kernel* aplicado (tamaño x tamaño)\n",
        "*   **stride**: desplazamiento de la aplicación del operador de convolución\n",
        "*   **padding**: tipo de padding applicado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FOqUpwekkyQZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "conv1 = nn.Conv2d(1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
        "\n",
        "print(conv1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJIlDKF2l5lZ"
      },
      "source": [
        "Posteriormente tenemos *capas con conexión completa* (*fully connected*) mediante la función ``nn.Linear``. Como comparación con la capa convolucional, en esta se asume que todas los datos de entrada están conectados a todos los datos de salida, hecho que aumenta significativamente el número de parámetros. La función tiene los principales argumentos:\n",
        "*   **in_features**: numero de unidades de entrada. \n",
        "*   **out_features**: numero de unidades de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KcjtXoj6mQHZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear(in_features=240, out_features=120, bias=True)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "fc1 = nn.Linear(in_features=240, out_features=120)\n",
        "\n",
        "print(fc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnPxR0_nmutN"
      },
      "source": [
        "También exite una etapa dedicada a reducir la dimensionalidad de los datos, cuya nomenclatura es ``nn.MaxPool2d``. Tiene los siguientes argumentos de interés:\n",
        "\n",
        "*   **kernel_size**: tamaño del *kernel* aplicado (tamaño x tamaño)\n",
        "*   **stride**: desplazamiento de la aplicación del operador de convolución\n",
        "*   **padding**: tipo de padding applicado\n",
        "\n",
        "Es importante resaltar el efecto de esta etapa. Por ejemplo con *kernel_size=2* y *stride=2*, estaremos reduciendo la dimensionalidad de la imagen por dos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H9q5LZ3DnLry"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "print(pool1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w9gOWoBoKjW"
      },
      "source": [
        "Por último se destaca la operación de activación, implementada mayoritariamente mediante funciones ``ReLU``. En el siguiente código se encuentra comentada debido a que, para ser efectiva, necesita datos 'x' a procesar. Estos datos 'x' se corresponderán con tensor, que no utilizan/cargan en el siguiente bloque de código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WdBqua3OoY5m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear(in_features=240, out_features=120, bias=True)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "fc1 = nn.Linear(in_features=240, out_features=120)\n",
        "#x = F.relu(fc1(x)) #fc1(x) corresponde con aplicar la capa fc1 a unos datos 'x'\n",
        "\n",
        "print(fc1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW-MovTyk3-q"
      },
      "source": [
        "### Definición red completa\n",
        "Tras las definiciones anteriores, estamos en condición de mostrar un esquema de la red LENET. \n",
        "\n",
        "Una definición básica requiere (al menos) implementar dos funciones:\n",
        "\n",
        "\n",
        "*   **__init__(self)**: indica el tipo de capas existentes en la red\n",
        "*   **forward**: indica el flujo de datos, es decir, la secuencia de ejecución cuando llegan nuevos datos a la entrada.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1l7o03z8kD1m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Convolutional layer 1\n",
        "        # 1-channel input image, 6 output channels, 5x5 square convolution \n",
        "        self.conv1 = nn.Conv2d(1, out_channels=6, kernel_size=5, stride=1, padding=0)\n",
        "        \n",
        "        # Max pooling over a (2, 2) window\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        # Convolutional layer 2\n",
        "        # 6-channel input data, 16 output channels, 5x5 square convolution \n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) \n",
        "        \n",
        "        # Fully connected neural network layers \n",
        "        # implement an affine operation: y = Wx + b\n",
        "        # where x-input\n",
        "        #       y-output\n",
        "        #       W-weights\n",
        "        #       b-bias\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) #fully connected layer 1\n",
        "        self.fc2 = nn.Linear(120, 84)         #fully connected layer 2\n",
        "        self.fc3 = nn.Linear(84, 10)          #fully connected layer 3\n",
        "        \n",
        "        #weight initializacion\n",
        "        #...\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        #layer 1: conv + ReLU + pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        #x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # definicion alternativa\n",
        "\n",
        "        #layer 2: conv + ReLU + pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        #x = F.max_pool2d(F.relu(self.conv2(x)), 2) #  definicion alternativa\n",
        "\n",
        "        #flatten data: convert 2D data into a 1D column vector\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        \n",
        "        #layer 3: fully connected\n",
        "        x = F.relu(self.fc1(x))\n",
        "        \n",
        "        #layer 4: fully connected\n",
        "        x = F.relu(self.fc2(x))\n",
        "        \n",
        "        #layer 5: fully connected\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92BcHzrjqjvY"
      },
      "source": [
        "NOTA1: observe que *out_channels* de la capa ``conv1`` debe coincidir con la variable *in_channels* de la capa ``conv2``.\n",
        "\n",
        "NOTA2: similarmente, la salida de la capa ``fc1`` coincide con la entrada de la capa ``fc2``, cuya salida tambien coincide con la entrada de la capa ``fc3``\n",
        "\n",
        "NOTA3: observe que la salida de la capa ``fc3`` es 10, el número de clases del problema de clasificación que se quiere resolver con la red LENET\n",
        "\n",
        "\n",
        "Por último, para utilizar esta red debemos crear un objeto de ella:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zCkgCBrxqksQ"
      },
      "outputs": [],
      "source": [
        "net = Net()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNrNgG6gqxm0"
      },
      "source": [
        "Adicionalmente podemos las capas que componen la red creada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LCAflIStq2ga"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssmo9JBssNzA"
      },
      "source": [
        "Y también podemos ver los parámetros que se pueden aprender mediante entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4SaimAl7sTqz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n",
            "torch.Size([16, 6, 5, 5])\n",
            "torch.Size([120, 400])\n",
            "torch.Size([84, 120])\n",
            "torch.Size([10, 84])\n"
          ]
        }
      ],
      "source": [
        "params = list(net.parameters())\n",
        "print(len(params))\n",
        "print(params[0].size())  # conv1's .weight\n",
        "print(params[2].size())  # conv2's .weight\n",
        "print(params[4].size())  # fc1's .weight\n",
        "print(params[6].size())  # fc2's .weight\n",
        "print(params[8].size())  # fc3's .weight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUkD4nHDsgSX"
      },
      "source": [
        "### Ejecución (forward pass)\n",
        "\n",
        "La ejecución de una red es sencilla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QT5b9VrOslRW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1313, -0.0510, -0.0849,  0.2580, -0.0406, -0.0416,  0.0711, -0.0018,\n",
            "         -0.0108,  0.0258]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "#create random data with 32x32 dimenssions\n",
        "input = Variable(torch.randn(1, 1, 32, 32))\n",
        "\n",
        "#process the data with the network\n",
        "out = net(input)\n",
        "\n",
        "#visualize network output\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRmZxod7tjPX"
      },
      "source": [
        "Cuando deba ejecutar su red, considere los siguientes puntos:\n",
        "\n",
        "\n",
        "*   Tamaño de entrada esperado: la red que acabamos de definir (Lenet) procesa imágenes de tamaño 32x32. Es decir, si quiere utilizar otro dataset deberá redimensionar las imágenes de entrada.\n",
        "*   *torch.nn* solamente procesa mini-batches de datos/imágenes (no imágenes individuales). Por ejemplo la función *nn.Conv2d* toma como entrada un tensor 4D ``nSamples x nChannels x Height x Width``\n",
        "*   La función **Variable** convierte los datos a procesar (tensores) en estructuras enriquecidas que permiten funcionalidades avanzadas (e.g. historial operaciones).\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Cm72BAOKs_Yq"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7ef080f1cd99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ],
      "source": [
        "net.zero_grad()\n",
        "out.backward(torch.randn(1, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBI7veQJuwFb"
      },
      "source": [
        "### Función de pérdidas (loss function)\n",
        "\n",
        "Una función de pérdida toma el par de entradas (salida, objetivo) y calcula un valor que calcula qué tan lejos está la salida del objetivo. Existen varias [funciones de pérdida](http://pytorch.org/docs/nn.html#loss-functions) en el paquete *nn*.\n",
        "\n",
        "Una pérdida simple es: `` nn.MSELoss`` que calcula el error cuadrático medio entre la entrada y el objetivo.\n",
        "Una pérdida simple es: `` nn.CrossEntropyLoss`` que calcula el error entropía cruzada entre la entrada y el objetivo.\n",
        "\n",
        "A continuación se muestra un ejemplo de ejecución\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JFCOWcBNu4QQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(38.3632, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#generate fake input/output\n",
        "output = net(input) \n",
        "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
        "target = target.to(torch.float32)\n",
        "\n",
        "#define criterion for loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "#apply loss function\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K70IjjOFs_C2"
      },
      "source": [
        "### Retropropagación (backpropagation)\n",
        "\n",
        "Para poder calcular el error cometido por la red en cada etapa, debemos ejecutar la red en modo inverso mediante la funcion `` loss.backward() ``. Esta función se calcular a partir de la función `` forward `` que hemos proporcionado en la definición de la red. \n",
        "\n",
        "Para retropropagar el error, han de seguirse los siguientes pasos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PxhGvQfHxfKP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.bias.grad before backward\n",
            "tensor([0., 0., 0., 0., 0., 0.])\n",
            "conv1.bias.grad after backward\n",
            "tensor([ 0.0184,  0.0432,  0.0604, -0.0113, -0.1083,  0.0823])\n"
          ]
        }
      ],
      "source": [
        "# You need to clear the existing gradients though, else gradients will be accumulated to existing gradients\n",
        "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
        "\n",
        "#have a look at conv1's bias gradients before and after the backward.\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA1qm-42xs2l"
      },
      "source": [
        "### Actualización de parámetros/pesos (update the weights)\n",
        "\n",
        "En esta parte procedemos a explicar muy brevement el proceso de actualización de los pesos.\n",
        "\n",
        "Primeramente necesitamos una herramienta de optimización. La ténica más sencilla y comúnmente utilizada es el \"descenso estocástico por gradiente\" (Stochastic Gradient\n",
        "Descent, SGD):\n",
        "\n",
        "     ``weight = weight - learning_rate * gradient``\n",
        "     \n",
        "Que podemos implementar fácilmente en Python\n",
        "\n",
        "    learning_rate = 0.01\n",
        "    for f in net.parameters():\n",
        "        f.data.sub_(f.grad.data * learning_rate)\n",
        "        \n",
        "No obstante, existen otras aproximaciones para el proceso de optimización: SGD, Nesterov-SGD, Adam, RMSProp,... que están contenidas en el paquete ``torch.optim``. El siguiente código muestra un ejemplo de utilización del optimizador.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yuQ636VtxxPN"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBllBDwhy8iS"
      },
      "source": [
        "## Bonus: definición de red con entrada genérica\n",
        "Como se ha comentado anteriormente, la red Lenet solamente puede trabajar con imágenes de gris y tamaño 32x32. En el siguiente bloque de código se muestra una adaptación de la red para procesar imágenes de mayor tamaño (en el ejemplo 224x224) y con varios canales de color (en el ejemplo 3). Observe que se ha añadido la función  ``_get_conv_output`` para calcular las dimensiones de los datos a la entrada de la capa fc1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BmRjmTMtzE8g"
      },
      "outputs": [],
      "source": [
        "#Original Lenet network\n",
        "#https://github.com/kuangliu/pytorch-cifar/tree/master/models\n",
        "\n",
        "#Possible extensions\n",
        "#https://discuss.pytorch.org/t/inferring-shape-via-flatten-operator/138/3\n",
        "#https://stackoverflow.com/questions/42479902/how-view-method-works-for-tensor-in-torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "  \n",
        "    #define the structure of the network\n",
        "    def __init__(self, input_shape=(3, 224, 224),num_outputs=15):\n",
        "        super(Net, self).__init__()\n",
        "        # Convolutional layer - 3 input image channel, 6 output channels, 5x5 square convolution \n",
        "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=6, kernel_size=5, stride=1, padding=0)\n",
        "        \n",
        "        # Max pooling over a (2, 2) window\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        # Convolutional layer - 6 input data channel, 16 output channels, 5x5 square convolution \n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)       \n",
        "\n",
        "        n_size = self._get_conv_output(input_shape)\n",
        "        \n",
        "        # Fully connected neural network layers       \n",
        "        self.fc1 = nn.Linear(in_features=n_size, out_features=120)\n",
        "        self.fc2 = nn.Linear(in_features=120,    out_features=84)\n",
        "        self.fc3 = nn.Linear(in_features=84,     out_features=num_outputs)\n",
        "\n",
        "    #define how data flows through the network\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        \n",
        "        #The view function is meant to reshape the tensor (flatten operator).\n",
        "        x = x.view( x.size(0),-1)\n",
        "                \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x \n",
        "        \n",
        "    def _get_conv_output(self, shape):\n",
        "        bs = 1\n",
        "        x = Variable(torch.rand(bs, *shape))\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        output_feat = self.pool(F.relu(self.conv2(x)))\n",
        "               \n",
        "        n_size = output_feat.data.view(bs, -1).size(1)\n",
        "        return n_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx95-m6B1fX7"
      },
      "source": [
        "A continuación, podemos crear una red para un problema dado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "buKPAQwj1gYc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=13456, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class_names = ('clase1', 'clase2', 'clase3')\n",
        "\n",
        "#creamos una red con \n",
        "# input = imágenes RGB de tamaño 128x128\n",
        "# output = tres clases\n",
        "net = Net(input_shape=(3,128,128), num_outputs=len(class_names))\n",
        "  \n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlSreI5J1tcb"
      },
      "source": [
        "Si cambiamos el problema, las dimensiones de la red cambian como puede observarse tras ejecutar ``print``:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ip3gY-Ct1xZ1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=250000, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "      \n",
        "classes = ('clase1', 'clase2', 'clase3', 'clase4', 'clase5')\n",
        "\n",
        "#creamos una red con \n",
        "# input = imágenes RGB de tamaño 128x128\n",
        "# output = tres clases\n",
        "net = Net(input_shape=(3,512,512), num_outputs=len(classes))\n",
        "  \n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.6 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "0773a600e78047011eb4ea94a1a46904bcfe3cc650f442fd5a25aa345eba1933"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
